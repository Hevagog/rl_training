{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from grid_environments.windy_gridworld import WindyGridWOrldEnv\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from algorithms.sarsa import Sarsa\n",
    "from algorithms.n_step_sarsa import NStepSarsa\n",
    "from grid_environments.cliff_env import CliffEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CliffWalking-v0\")\n",
    "env = make_vec_env(\"CliffWalking-v0\", n_envs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_n_step = NStepSarsa(\n",
    "        gamma=0.9,\n",
    "        alpha=0.2,\n",
    "        epsilon_0=1.0,\n",
    "        epsilon_min=0.01,\n",
    "        decay_rate=0.0005,\n",
    "        action_space=env.action_space.n,\n",
    "        observation_space=env.observation_space.n,\n",
    "        n=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43magent_n_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pytorch_learning/rl_training/custom_environments/gridworld_simple/algorithms/n_step_sarsa.py:67\u001b[0m, in \u001b[0;36mNStepSarsa.train\u001b[0;34m(self, env, episodes, plot_every, decay_epsilon)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tau \u001b[38;5;241m<\u001b[39m T \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m<\u001b[39m T:\n\u001b[0;32m---> 67\u001b[0m         next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m         episode_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     69\u001b[0m         n_step_buffer\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/pytorch_learning/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pytorch_learning/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m     58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(\n\u001b[0;32m---> 59\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "history = agent_n_step.train(env, episodes=100_000, decay_epsilon=True, plot_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Sarsa(\n",
    "        gamma=0.9,\n",
    "        alpha=0.1,\n",
    "        epsilon_0=1.0,\n",
    "        epsilon_min=0.01,\n",
    "        decay_rate=0.0005,\n",
    "        action_space=env.action_space.n,\n",
    "        observation_space=env.observation_space.n,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Score: -7765.0\n",
      "Episode: 1000, Score: -11244.185\n",
      "Episode: 2000, Score: -1515.784\n",
      "Episode: 3000, Score: -435.987\n",
      "Episode: 4000, Score: -29.311\n",
      "Episode: 5000, Score: -22.71\n",
      "Episode: 6000, Score: -21.418\n",
      "Episode: 7000, Score: -19.133\n",
      "Episode: 8000, Score: -18.33\n",
      "Episode: 9000, Score: -17.925\n",
      "Episode: 10000, Score: -17.471\n",
      "Episode: 11000, Score: -17.429\n",
      "Episode: 12000, Score: -17.638\n",
      "Episode: 13000, Score: -17.38\n",
      "Episode: 14000, Score: -17.599\n",
      "Episode: 15000, Score: -17.324\n",
      "Episode: 16000, Score: -17.6\n",
      "Episode: 17000, Score: -17.46\n",
      "Episode: 18000, Score: -17.381\n",
      "Episode: 19000, Score: -17.379\n",
      "Episode: 20000, Score: -17.359\n",
      "Episode: 21000, Score: -17.268\n",
      "Episode: 22000, Score: -17.288\n",
      "Episode: 23000, Score: -17.685\n",
      "Episode: 24000, Score: -17.397\n",
      "Episode: 25000, Score: -17.274\n",
      "Episode: 26000, Score: -17.695\n",
      "Episode: 27000, Score: -17.291\n",
      "Episode: 28000, Score: -17.354\n",
      "Episode: 29000, Score: -17.773\n",
      "Episode: 30000, Score: -17.379\n",
      "Episode: 31000, Score: -17.595\n",
      "Episode: 32000, Score: -17.275\n",
      "Episode: 33000, Score: -17.386\n",
      "Episode: 34000, Score: -17.863\n",
      "Episode: 35000, Score: -17.494\n",
      "Episode: 36000, Score: -17.376\n",
      "Episode: 37000, Score: -17.555\n",
      "Episode: 38000, Score: -17.361\n",
      "Episode: 39000, Score: -17.512\n",
      "Episode: 40000, Score: -17.455\n",
      "Episode: 41000, Score: -17.385\n",
      "Episode: 42000, Score: -17.185\n",
      "Episode: 43000, Score: -17.595\n",
      "Episode: 44000, Score: -17.787\n",
      "Episode: 45000, Score: -17.505\n",
      "Episode: 46000, Score: -17.576\n",
      "Episode: 47000, Score: -17.459\n",
      "Episode: 48000, Score: -17.164\n",
      "Episode: 49000, Score: -17.394\n",
      "Episode: 50000, Score: -17.171\n",
      "Episode: 51000, Score: -17.482\n",
      "Episode: 52000, Score: -17.809\n",
      "Episode: 53000, Score: -17.398\n",
      "Episode: 54000, Score: -17.374\n",
      "Episode: 55000, Score: -17.565\n",
      "Episode: 56000, Score: -17.275\n",
      "Episode: 57000, Score: -17.474\n",
      "Episode: 58000, Score: -17.594\n",
      "Episode: 59000, Score: -17.188\n",
      "Episode: 60000, Score: -17.477\n",
      "Episode: 61000, Score: -17.271\n",
      "Episode: 62000, Score: -17.384\n",
      "Episode: 63000, Score: -17.558\n",
      "Episode: 64000, Score: -17.874\n",
      "Episode: 65000, Score: -17.357\n",
      "Episode: 66000, Score: -17.365\n",
      "Episode: 67000, Score: -17.292\n",
      "Episode: 68000, Score: -17.394\n",
      "Episode: 69000, Score: -17.273\n",
      "Episode: 70000, Score: -17.278\n",
      "Episode: 71000, Score: -17.491\n",
      "Episode: 72000, Score: -17.47\n",
      "Episode: 73000, Score: -17.589\n",
      "Episode: 74000, Score: -17.593\n",
      "Episode: 75000, Score: -17.282\n",
      "Episode: 76000, Score: -17.387\n",
      "Episode: 77000, Score: -17.27\n",
      "Episode: 78000, Score: -17.382\n",
      "Episode: 79000, Score: -17.302\n",
      "Episode: 80000, Score: -17.402\n",
      "Episode: 81000, Score: -17.378\n",
      "Episode: 82000, Score: -17.176\n",
      "Episode: 83000, Score: -17.689\n",
      "Episode: 84000, Score: -17.38\n",
      "Episode: 85000, Score: -17.593\n",
      "Episode: 86000, Score: -17.73\n",
      "Episode: 87000, Score: -17.309\n",
      "Episode: 88000, Score: -17.686\n",
      "Episode: 89000, Score: -17.287\n",
      "Episode: 90000, Score: -17.465\n",
      "Episode: 91000, Score: -17.427\n",
      "Episode: 92000, Score: -17.268\n",
      "Episode: 93000, Score: -17.591\n",
      "Episode: 94000, Score: -17.271\n",
      "Episode: 95000, Score: -17.202\n",
      "Episode: 96000, Score: -17.476\n",
      "Episode: 97000, Score: -17.387\n",
      "Episode: 98000, Score: -17.483\n",
      "Episode: 99000, Score: -17.682\n"
     ]
    }
   ],
   "source": [
    "history = agent.train(env, episodes=100_000, decay_epsilon=True, plot_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_sarsa_n_step = np.array(\n",
    "        [\n",
    "            np.argmax(agen_n_step.q_table[key]) \n",
    "            for key in np.arange(48)\n",
    "        ]\n",
    ").reshape(4, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1],\n",
       "       [3, 1, 1, 1, 0, 2, 0, 1, 1, 3, 1, 2],\n",
       "       [1, 0, 0, 0, 0, 3, 1, 1, 3, 0, 1, 2],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_sarsa_n_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_sarsa = np.array(\n",
    "        [\n",
    "            np.argmax(agent.q_table[key]) \n",
    "            for key in np.arange(48)\n",
    "        ]\n",
    ").reshape(4, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cliff = CliffEnv(render_mode=\"rgb_array\", size_x=6, size_y=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = Sarsa(\n",
    "        gamma=0.9,\n",
    "        alpha=0.1,\n",
    "        epsilon_0=1.0,\n",
    "        epsilon_min=0.01,\n",
    "        decay_rate=0.0005,\n",
    "        action_space=env_cliff.action_space.n,\n",
    "        observation_space=env_cliff.observation_space.n,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Score: -100.0\n",
      "Episode: 1000, Score: -94.592\n",
      "Episode: 2000, Score: -35.43\n",
      "Episode: 3000, Score: -18.532\n",
      "Episode: 4000, Score: -9.85\n",
      "Episode: 5000, Score: -4.797\n",
      "Episode: 6000, Score: -2.262\n",
      "Episode: 7000, Score: -2.175\n",
      "Episode: 8000, Score: -1.26\n",
      "Episode: 9000, Score: -0.984\n",
      "Episode: 10000, Score: -0.711\n",
      "Episode: 11000, Score: -0.259\n",
      "Episode: 12000, Score: -0.458\n",
      "Episode: 13000, Score: -0.32\n",
      "Episode: 14000, Score: -0.609\n",
      "Episode: 15000, Score: -0.515\n",
      "Episode: 16000, Score: -0.425\n",
      "Episode: 17000, Score: -0.422\n",
      "Episode: 18000, Score: -0.401\n",
      "Episode: 19000, Score: -0.414\n",
      "Episode: 20000, Score: -0.534\n",
      "Episode: 21000, Score: -0.604\n",
      "Episode: 22000, Score: -0.438\n",
      "Episode: 23000, Score: -0.137\n",
      "Episode: 24000, Score: -0.409\n",
      "Episode: 25000, Score: -0.657\n",
      "Episode: 26000, Score: -0.442\n",
      "Episode: 27000, Score: -0.424\n",
      "Episode: 28000, Score: -0.117\n",
      "Episode: 29000, Score: -0.385\n",
      "Episode: 30000, Score: -0.116\n",
      "Episode: 31000, Score: -0.502\n",
      "Episode: 32000, Score: -0.622\n",
      "Episode: 33000, Score: -0.409\n",
      "Episode: 34000, Score: -0.335\n",
      "Episode: 35000, Score: -0.325\n",
      "Episode: 36000, Score: -0.125\n",
      "Episode: 37000, Score: -0.218\n",
      "Episode: 38000, Score: -0.418\n",
      "Episode: 39000, Score: -0.085\n",
      "Episode: 40000, Score: -0.422\n",
      "Episode: 41000, Score: -0.413\n",
      "Episode: 42000, Score: -0.206\n",
      "Episode: 43000, Score: -0.114\n",
      "Episode: 44000, Score: -0.531\n",
      "Episode: 45000, Score: -0.43\n",
      "Episode: 46000, Score: -0.247\n",
      "Episode: 47000, Score: -0.329\n",
      "Episode: 48000, Score: -0.519\n",
      "Episode: 49000, Score: -0.642\n",
      "Episode: 50000, Score: -0.203\n",
      "Episode: 51000, Score: -0.42\n",
      "Episode: 52000, Score: -0.447\n",
      "Episode: 53000, Score: -0.29\n",
      "Episode: 54000, Score: -0.438\n",
      "Episode: 55000, Score: -0.314\n",
      "Episode: 56000, Score: -0.722\n",
      "Episode: 57000, Score: -0.304\n",
      "Episode: 58000, Score: -0.432\n",
      "Episode: 59000, Score: -0.402\n",
      "Episode: 60000, Score: -0.459\n",
      "Episode: 61000, Score: -0.425\n",
      "Episode: 62000, Score: -0.604\n",
      "Episode: 63000, Score: -0.403\n",
      "Episode: 64000, Score: -0.337\n",
      "Episode: 65000, Score: -0.207\n",
      "Episode: 66000, Score: -0.209\n",
      "Episode: 67000, Score: -0.226\n",
      "Episode: 68000, Score: -0.825\n",
      "Episode: 69000, Score: -0.341\n",
      "Episode: 70000, Score: -0.523\n",
      "Episode: 71000, Score: -0.209\n",
      "Episode: 72000, Score: -0.513\n",
      "Episode: 73000, Score: -0.316\n",
      "Episode: 74000, Score: -0.426\n",
      "Episode: 75000, Score: -0.23\n",
      "Episode: 76000, Score: -0.119\n",
      "Episode: 77000, Score: -0.822\n",
      "Episode: 78000, Score: -0.09\n",
      "Episode: 79000, Score: -0.327\n",
      "Episode: 80000, Score: -0.412\n",
      "Episode: 81000, Score: -0.124\n",
      "Episode: 82000, Score: -0.222\n",
      "Episode: 83000, Score: -0.213\n",
      "Episode: 84000, Score: -0.231\n",
      "Episode: 85000, Score: -0.414\n",
      "Episode: 86000, Score: -0.508\n",
      "Episode: 87000, Score: -0.501\n",
      "Episode: 88000, Score: -0.242\n",
      "Episode: 89000, Score: -0.524\n",
      "Episode: 90000, Score: -0.235\n",
      "Episode: 91000, Score: -0.436\n",
      "Episode: 92000, Score: -0.322\n",
      "Episode: 93000, Score: -0.411\n",
      "Episode: 94000, Score: -0.201\n",
      "Episode: 95000, Score: -0.323\n",
      "Episode: 96000, Score: -0.333\n",
      "Episode: 97000, Score: -0.648\n",
      "Episode: 98000, Score: -0.331\n",
      "Episode: 99000, Score: -0.116\n"
     ]
    }
   ],
   "source": [
    "history_2 = agent2.train(env_cliff, episodes=100_000, decay_epsilon=True, plot_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent2.q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_sarsa_2 = np.array(\n",
    "        [\n",
    "            np.argmax(agent2.q_table[key]) \n",
    "            for key in np.arange(30)\n",
    "        ]\n",
    ").reshape(6,5)\n",
    "policy_sarsa_2[env_cliff._cliff] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  0,  0,  0],\n",
       "       [-1,  1,  1,  0,  0],\n",
       "       [-1,  1,  1,  0,  0],\n",
       "       [-1,  1,  0,  0,  0],\n",
       "       [-1,  0,  0,  0,  0],\n",
       "       [ 0,  3,  3,  3,  3]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_sarsa_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
